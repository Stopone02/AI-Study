{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ae0ade-b79d-4bb5-8ccc-8f64c1a1bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\"\"\" 원래 크롤링이 포함된 프로젝트이지만, 청원 서비스 개편으로 아래 코드 사용 X\n",
    "result = pd.DataFrame()\n",
    "for i in range(584274, 595226):\n",
    "    URL = \"http://www1.president.go.kr/petitions/\"+str(i)\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.find('h3', class_='petitionsView_title')\n",
    "    count = soup.find('span', class_='counter')\n",
    "\n",
    "    for content in soup.select('div.petitionsView_write > div.View_write'):\n",
    "        content\n",
    "\n",
    "    a = []\n",
    "    for tag in soup.select('ul.petitionsView_info_list >li'):\n",
    "        a.append(tag.contents[1])\n",
    "\n",
    "    if len(a) != 0:\n",
    "        df1 = pd.DataFrame({'start' : [a[1]], 'end' : [a[2]], 'category' : [a[0]], 'count' : [count.text], 'title' : [title.text], 'content' : [content.text.strip()[0: 13000]]})\n",
    "        result = pd.concat([result, df1])\n",
    "        result.index = np.arange(len(result))\n",
    "\n",
    "    if i % 60 == 0:\n",
    "        print(\"Sleep 90seconds. Count:\" + str(i) + \", Local Time:\" + time.strftime('%Y-%m-%d', time.localtime(time.time())) + \" \" + time.strftime('%X', time.localtime(time.time())) + \", Data Length:\" + str(len(result)))\n",
    "        time.sleep(90)\n",
    "\n",
    "df = result\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('./crawling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9087ab22-7cbf-4912-8eca-e8f1177a43c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리 나라 코스피 시총이 미국 애플보다 작다는 설이 돌 정도로 한국의 주식시장은 저평가된 시장이라고 합니다. 하지만 투자매력이 없다고도 합니다.이렇게 말하는 이유가 어디 있습니까? 바로 투자를 해도 수익을 기대하기 어렵다는 인식이 이미 널리 퍼져 있다는 것입니다.\\r\\n\\r\\r\\n지금은 투자매력이 없어서 그렇지,..우리 나라 시중 부동 자금이 어마어마 한 것으로 알려진 것과, 외국 투자 자본 또한 실로 어마무지하게 많다는 것도 알고있습니다. 그러나 이 투자금이 주식시장으로 원활하게 순환이 안되고 있다는데 있습니다.\\r\\n\\r\\r\\n정부는 유휴자금이 주식 시장으로 들어오게 분위기를 띄어줘야 하는데,... 그렇지 못한 현실이 안타깝다고 생각합니다.\\r\\r\\n국가가 지원해 돈 들이기가 어려우면,정부에서 정서적인 말이라도 활성화를 위한 관심표명과 정책적으로 지원만 해도 시장분위기는 많이 좋아질 것이라 확신합니다. \\r\\n\\r\\r\\n그래서 저는 정부에 다음과 같이 청원합니다.\\r\\r\\n주식시장에 더 큰 충격 오기 전에 정부가 예방 조치를 할 수 있는데까지 노력해 주시기를 당부 드리면서,...\\r\\n\\r\\r\\n첫째,주식시장 활성화와 부양에 대한 정부의 의지를 표명해 주십시오 \\r\\n\\r\\r\\n둘째,  코스닥 종목은 공매도 제외시켜 주세요.\\r\\n\\r\\r\\n공매도 제도를 아예 없애버리면 좋겠지만 제도를 살린다면 적용 시장에서 코스닥 종목은 제외 해 주어야 체력이 약한 코스닥 시장을 안정시킬 수 있다고 봅니다 \\r\\n\\r\\r\\n셋째, 거래시장의 주식거래세를 더 낮춰 주세요 \\r\\n\\r\\r\\n특히 개미(소액)투자에 동일한 요율로 부과하는 것은 너무나 불공정 하다고 봅니다.\\r\\n\\r\\r\\n넷째, 중,장기 보유자에 대한 우대 차원에서 보유기간을 설정하여 주세요.\\r\\n\\r\\r\\n이는 장기 보유자에 대한 혜택을 주어 초단타매매 등 시장 질서를 교란하는 투자자와 분리하는 제도를 도입해 시장 안정화에 기여해 주시길 간곡히 부탁드리면서 청원합니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a1158ed-5a7a-482c-a0b0-84a6dcff26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "import re\n",
    "\n",
    "def remove_white_space(text):\n",
    "    text = re.sub(r'[\\t\\r\\n\\f\\v]', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "def remove_special_char(text):\n",
    "    text = re.sub('[^ ㄱ-ㅣ가-힣 0-9]+', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "df.title = df.title.apply(remove_white_space)\n",
    "df.title = df.title.apply(remove_special_char)\n",
    "\n",
    "df.content = df.content.apply(remove_white_space)\n",
    "df.content = df.content.apply(remove_special_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837b6894-bec3-45cb-a16b-5fdff8875486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리 나라 코스피 시총이 미국 애플보다 작다는 설이 돌 정도로 한국의 주식시장은 저평가된 시장이라고 합니다  하지만 투자매력이 없다고도 합니다 이렇게 말하는 이유가 어디 있습니까  바로 투자를 해도 수익을 기대하기 어렵다는 인식이 이미 널리 퍼져 있다는 것입니다      지금은 투자매력이 없어서 그렇지 우리 나라 시중 부동 자금이 어마어마 한 것으로 알려진 것과  외국 투자 자본 또한 실로 어마무지하게 많다는 것도 알고있습니다  그러나 이 투자금이 주식시장으로 원활하게 순환이 안되고 있다는데 있습니다      정부는 유휴자금이 주식 시장으로 들어오게 분위기를 띄어줘야 하는데  그렇지 못한 현실이 안타깝다고 생각합니다    국가가 지원해 돈 들이기가 어려우면 정부에서 정서적인 말이라도 활성화를 위한 관심표명과 정책적으로 지원만 해도 시장분위기는 많이 좋아질 것이라 확신합니다       그래서 저는 정부에 다음과 같이 청원합니다    주식시장에 더 큰 충격 오기 전에 정부가 예방 조치를 할 수 있는데까지 노력해 주시기를 당부 드리면서      첫째 주식시장 활성화와 부양에 대한 정부의 의지를 표명해 주십시오      둘째   코스닥 종목은 공매도 제외시켜 주세요      공매도 제도를 아예 없애버리면 좋겠지만 제도를 살린다면 적용 시장에서 코스닥 종목은 제외 해 주어야 체력이 약한 코스닥 시장을 안정시킬 수 있다고 봅니다      셋째  거래시장의 주식거래세를 더 낮춰 주세요      특히 개미 소액 투자에 동일한 요율로 부과하는 것은 너무나 불공정 하다고 봅니다      넷째  중 장기 보유자에 대한 우대 차원에서 보유기간을 설정하여 주세요      이는 장기 보유자에 대한 혜택을 주어 초단타매매 등 시장 질서를 교란하는 투자자와 분리하는 제도를 도입해 시장 안정화에 기여해 주시길 간곡히 부탁드리면서 청원합니다 '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8425994b-2300-4068-86d6-b43eda1674f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "df['title_token'] = df.title.apply(okt.morphs)\n",
    "df['content_token'] = df.content.apply(okt.nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7efa07-8451-4daf-8fa7-7fbdd8c24286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category         object\n",
      "content          object\n",
      "count             int64\n",
      "end              object\n",
      "start            object\n",
      "title            object\n",
      "title_token      object\n",
      "content_token    object\n",
      "token_final      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 파생변수 생성\n",
    "df['token_final'] = df.title_token + df.content_token\n",
    "\n",
    "df['count'] = df['count'].replace({',' : ''}, regex = True).apply(lambda x : int(x))\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "df['label'] = df['count'].apply(lambda x: 'Yes' if x>=1000 else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01c0290d-c3e9-4095-a9aa-fa09b69a40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df[['token_final', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02b2ced-43e7-4eb7-b8a3-01ca17e6a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIWON\\anaconda3\\envs\\speech\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=42642, size=100, alpha=0.025)\n",
      "[('음주', 0.8649716973304749), ('무면허', 0.8320099115371704), ('뺑소니', 0.8231046199798584), ('전과자', 0.7831006050109863), ('살인자', 0.7534054517745972), ('살인죄', 0.7516147494316101), ('윤창', 0.7497144341468811), ('형량', 0.7398381233215332), ('승자', 0.7391788959503174), ('스토킹', 0.7385151386260986)]\n"
     ]
    }
   ],
   "source": [
    "# 단어 임베딩\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_model = Word2Vec(df_drop['token_final'], sg = 1, size = 100, window = 2, min_count = 1, workers = 4)\n",
    "print(embedding_model)\n",
    "\n",
    "model_result = embedding_model.wv.most_similar(\"음주운전\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6ebae4-82e7-4ea9-840c-f408b26db29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('음주', 0.8649716973304749), ('무면허', 0.8320099115371704), ('뺑소니', 0.8231046199798584), ('전과자', 0.7831006050109863), ('살인자', 0.7534054517745972), ('살인죄', 0.7516147494316101), ('윤창', 0.7497144341468811), ('형량', 0.7398381233215332), ('승자', 0.7391788959503174), ('스토킹', 0.7385151386260986)]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 저장 및 로드\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embedding_model.wv.save_word2vec_format('data/petitions_tokens_w2v')\n",
    "\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"data/petitions_tokens_w2v\")\n",
    "\n",
    "model_result = loaded_model.most_similar(\"음주운전\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "548adf75-4628-4df2-a17d-bf510464ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할 및 저장\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rng = RandomState()\n",
    "\n",
    "tr = df_drop.sample(frac=0.8, random_state=rng)\n",
    "val = df_drop.loc[~df_drop.index.isin(tr.index)]\n",
    "\n",
    "tr.to_csv('data/train.csv', index=False, encoding='utf-8-sig')\n",
    "val.to_csv('data/validation.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1bb2d84-d473-4112-8525-01c9c7bb7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field 클래스 정의\n",
    "import torchtext\n",
    "from torchtext.data import Field\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('[\\[\\]\\']', '', str(text))\n",
    "    text = text.split(', ')\n",
    "    return text\n",
    "\n",
    "TEXT = Field(tokenize=tokenizer)\n",
    "LABEL = Field(sequential = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "446223d5-fa45-4076-822a-e519fb9bab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ['유흥주점', '및', '시설', '전국', '적', '으로', '강제', '집합', '금지', '명령', '내려주세요', '코로나', '몇개', '월', '제대로', '생활', '못', '사람', '고통', '경제', '피해', '위기', '속', '국민', '위기', '의식', '가지', '사회', '거리', '두기', '코로나', '거의', '종식', '무렵', '정말', '개념', '사람', '그것', '음주', '가무', '위해', '사회', '거리', '두기', '무시', '집단', '발병', '발생', '용인', '번', '환자', '뉴스', '순간', '현장', '일선', '분투', '의료', '진', '화가', '또한', '무급', '휴가', '현재', '서울시', '종', '유흥', '시설', '강제', '집합', '금지', '명령', '전국', '유흥', '시설', '집합', '금지', '명령'] Yes\n",
      "Validation: ['서울', '지방', '병무청', '탈의실', '에', '설치', '된', '에', '대한', '진상', '규명', '을', '요구', '한', '다', '또한', '인권위', '의', '미온', '적', '대응', '을', '규탄', '한다', '본인', '경', '서울', '지방', '병무청', '제', '검사', '탈의실', '수', '것', '발견', '탈의실', '천장', '를', '발견', '것', '본인', '이', '경악', '탈의실', '를', '설치', '것', '개인정보보호법', '항', '위반', '사안', '법적', '문제', '촬영', '성폭력', '범죄', '처벌', '등', '관', '특례법', '항', '위반', '또한', '법적', '제일', '뿐', '개인', '자유', '침해', '성적', '수치심', '매우', '윤리', '문제', '불법', '카메라', '그', '자체', '문제', '공공기관', '탈의실', '불법', '비', '윤리', '카메라', '설치', '운영', '상황', '발생', '도대체', '한국', '정부', '인권', '무엇', '생각', '것', '발견', '직후', '인권위', '진정', '인권위', '현재', '조사', '진행', '답변', '병무청', '답변', '더', '이상', '답변', '것', '의미', '생각', '본인', '모', '커뮤니티', '이', '사실', '이후', '이슈', '화가', '병무청', '운영', '대답', '이', '대한', '조사', '경과', '발표', '사과', '책임', '대응', '전혀', '개인', '자유', '권리', '본질', '내용', '침해', '강제', '징집', '국제노동기구', '인정', '강제', '징용', '강제노동', '남성', '국가', '신체', '감시', '사건', '한국', '정부', '병역의무', '인권', '생각', '언제', '국가', '개인', '노예', '취급', '것', '백원', '시급', '강제', '폭력', '속', '것', '모자라', '이제', '어처구니', '일', '것', '경악', '수', '이', '본인', '위', '사실', '규탄', '아래', '대응', '요구', '서울', '지방', '병무청', '징병검사', '소', '탈의실', '설치', '경위', '운영', '여부', '관리', '역', '등', '대해', '낱낱이', '조사', '그', '진상', '병무청장', '지방', '병무청장', '이', '사건', '책임', '지고', '사퇴', '하라', '자진사퇴', '거부', '정부', '이', '파면', '이', '사건', '관련', '책임자', '담당자', '처벌', '병무', '정장', '명의', '사과문', '병무청', '사이트', '비롯', '온라인과', '전국', '신체검사', '소', '신체검사', '대상', '그', '사과', '내용', '알게', '사과', '내용', '조사', '경과', '언론', '배포', '이유', '인권위', '위', '민원', '무시', '조사', '책임자', '문책', '병무청', '제보자', '민원', '보복', '감시', '제보자', '민원', '적극', '보호', '한국', '정부', '남성', '강제', '징집', '강제노동', '대해', '최저임금', '배', '이상', '시급', '지급', '내', '현재', '징병제', '개혁', '강제', '징집', '강제', '징용', '강제노동', '폐지', '또한', '지금', '강제', '피해', '당한', '당사자', '사과', '합당', '피해', '보상금', '제공', '각주', '개인정보보호법', '제', '항', '제', '영상', '정보처리', '기기', '설치', '운영', '제한', '누구', '불', '특정', '다수', '이용', '목욕실', '화장실', '발', '한실', '탈의실', '등', '개인', '사생활', '현저', '침해', '우려', '장소', '내부', '볼', '수', '영상', '정보처리', '기기', '설치', '운영', '다만', '교도소', '정신', '보건', '시설', '등', '법령', '근거', '사람', '구금', '거나', '보호', '시설', '로서', '대통령령', '정', '시설', '대하', '여', '각주', '개인정보보호법', '제', '과태료', '항', '호', '다음', '각', '호의', '하나', '해당', '자', '이하', '과태료', '부과', '제', '항', '위반', '영상', '정보처리', '기기', '설치', '운영', '자', '각주', '성폭력', '범죄', '처벌', '등', '관', '특례법', '제', '카메라', '등', '이용', '촬영', '항', '카메라', '그', '이', '기능', '기계', '장치', '이용', '성적', '욕망', '수치심', '유발', '수', '사람', '신체', '촬영', '대상자', '의사', '촬영', '이하', '징역', '이하', '벌금', '처', '개정', '각주', '국제노동기구', '제', '호', '강제', '근로', '협약', '제', '호', '강제', '근로', '폐지', '협약', '제', '호', '정치', '견해', '기존', '정치', '사회', '제도', '사상', '반대', '견해', '발표', '것', '대한', '제재', '및', '정치', '억압', '교육', '수단', '나', '발전', '목적', '노동', '동원', '이용', '경우', '노동', '규제', '수단', '파업', '참가', '대한', '제재', '마', '인종', '사회', '민족', '종교', '차별', '대우', '수단'] No\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train, validation = TabularDataset.splits(\n",
    "    path = 'data/',\n",
    "    train = 'train.csv',\n",
    "    validation = 'validation.csv',\n",
    "    format = 'csv',\n",
    "    fields = [('text', TEXT), ('label', LABEL)],\n",
    "    skip_header = True)\n",
    "\n",
    "print(\"Train:\", train[0].text, train[0].label)\n",
    "print(\"Validation:\", validation[0].text, validation[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b09e227-fee0-4b70-a5d5-97fd94a325dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/42642 [00:00<?, ?it/s]Skipping token b'42642' with 1-dimensional vector [b'100']; likely a header\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 42642/42642 [00:01<00:00, 23243.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수와 차원 : torch.Size([39041, 100]) \n"
     ]
    }
   ],
   "source": [
    "# 단어장 및 DataLoader 정의\n",
    "import torch\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "vectors = Vectors(name='data/petitions_tokens_w2v')\n",
    "\n",
    "TEXT.build_vocab(train, vectors = vectors, min_freq = 1, max_size = None)\n",
    "\n",
    "LABEL.build_vocab(train, vectors = vectors, min_freq = 1, max_size = None)\n",
    "\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_iter, validation_iter = BucketIterator.splits(\n",
    "    datasets = (train, validation),\n",
    "    batch_size = 8,\n",
    "    device = device,\n",
    "    sort = False)\n",
    "\n",
    "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ead7af09-c39c-4616-ad24-5c5f39623091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN 모델링\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "\n",
    "        con_x = [self.relu(conv(emb_x)) for conv in self.convs]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "\n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "        fc_x = self.dropout(fc_x)\n",
    "\n",
    "        logit = self.fc(fc_x)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85ccfab7-de3f-46ee-afe1-845d73448ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_itr, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    corrects, train_loss = 0.0,0\n",
    "\n",
    "    for batch in train_itr:\n",
    "\n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(text)\n",
    "\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "\n",
    "    train_loss /= len(train_itr.dataset)\n",
    "    accuracy = 100.0 * corrects / len(train_itr.dataset)\n",
    "\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdda0b77-1390-4561-9ae4-491a79fb7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수 정의\n",
    "def evaluate(model, device, itr):\n",
    "\n",
    "    model.eval()\n",
    "    corrects, test_loss = 0.0, 0\n",
    "\n",
    "    for batch in itr:\n",
    "\n",
    "        text = batch.text\n",
    "        target = batch.label\n",
    "        text = torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "\n",
    "        logit = model(text)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "\n",
    "    test_loss /= len(itr.dataset)\n",
    "    accuracy = 100.0 * corrects / len(itr.dataset)\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c72253-c0de-4566-bd19-5fe4392214c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embed): Embedding(39041, 100)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 10, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 10, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 \t Loss: 0.08202454345381034 \t Accuracy: 61.562320709228516%\n",
      "Valid Epoch: 1 \t Loss: 0.07744564340614221 \t Accuracy: 65.57904815673828%\n",
      "model saves at 65.57904815673828 accuracy\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 성능 확인\n",
    "model = TextCNN(vocab, 100, 10, [3, 4, 5], 2).to(device)\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_test_acc = -1\n",
    "\n",
    "for epoch in range(1, 3+1):\n",
    "\n",
    "    tr_loss, tr_acc = train(model, device, train_iter, optimizer)\n",
    "\n",
    "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, device, validation_iter)\n",
    "\n",
    "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, val_loss, val_acc))\n",
    "\n",
    "    if val_acc > best_test_acc:\n",
    "        best_test_acc = val_acc\n",
    "\n",
    "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
    "        torch.save(model.state_dict(), \"TextCNN_Best_Validation\")\n",
    "\n",
    "    print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5172d1-d7f4-4558-8b79-591d474e681d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
