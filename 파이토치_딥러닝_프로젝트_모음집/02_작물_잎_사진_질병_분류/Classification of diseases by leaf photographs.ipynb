{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf826a8-5740-4425-96a4-fc5481885bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할을 위한 폴더 생성\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "original_dataset_dir = './dataset'\n",
    "classes_list = os.listdir(original_dataset_dir)\n",
    "\n",
    "base_dir = './splitted'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "for clss in classes_list:\n",
    "    os.mkdir(os.path.join(train_dir, clss))\n",
    "    os.mkdir(os.path.join(validation_dir, clss))\n",
    "    os.mkdir(os.path.join(test_dir, clss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f844de-0f3b-4aa2-93ca-6e6f600feaa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size( Apple___Apple_scab ):  378\n",
      "Validation size( Apple___Apple_scab ):  126\n",
      "Test size( Apple___Apple_scab ):  126\n",
      "Train size( Apple___Black_rot ):  372\n",
      "Validation size( Apple___Black_rot ):  124\n",
      "Test size( Apple___Black_rot ):  124\n",
      "Train size( Apple___Cedar_apple_rust ):  165\n",
      "Validation size( Apple___Cedar_apple_rust ):  55\n",
      "Test size( Apple___Cedar_apple_rust ):  55\n",
      "Train size( Apple___healthy ):  987\n",
      "Validation size( Apple___healthy ):  329\n",
      "Test size( Apple___healthy ):  329\n",
      "Train size( Cherry___healthy ):  512\n",
      "Validation size( Cherry___healthy ):  170\n",
      "Test size( Cherry___healthy ):  170\n",
      "Train size( Cherry___Powdery_mildew ):  631\n",
      "Validation size( Cherry___Powdery_mildew ):  210\n",
      "Test size( Cherry___Powdery_mildew ):  210\n",
      "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
      "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Train size( Corn___Common_rust ):  715\n",
      "Validation size( Corn___Common_rust ):  238\n",
      "Test size( Corn___Common_rust ):  238\n",
      "Train size( Corn___healthy ):  697\n",
      "Validation size( Corn___healthy ):  232\n",
      "Test size( Corn___healthy ):  232\n",
      "Train size( Corn___Northern_Leaf_Blight ):  591\n",
      "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
      "Test size( Corn___Northern_Leaf_Blight ):  197\n",
      "Train size( Grape___Black_rot ):  708\n",
      "Validation size( Grape___Black_rot ):  236\n",
      "Test size( Grape___Black_rot ):  236\n",
      "Train size( Grape___Esca_(Black_Measles) ):  829\n",
      "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
      "Test size( Grape___Esca_(Black_Measles) ):  276\n",
      "Train size( Grape___healthy ):  253\n",
      "Validation size( Grape___healthy ):  84\n",
      "Test size( Grape___healthy ):  84\n",
      "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
      "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Train size( Peach___Bacterial_spot ):  1378\n",
      "Validation size( Peach___Bacterial_spot ):  459\n",
      "Test size( Peach___Bacterial_spot ):  459\n",
      "Train size( Peach___healthy ):  216\n",
      "Validation size( Peach___healthy ):  72\n",
      "Test size( Peach___healthy ):  72\n",
      "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
      "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Train size( Pepper,_bell___healthy ):  886\n",
      "Validation size( Pepper,_bell___healthy ):  295\n",
      "Test size( Pepper,_bell___healthy ):  295\n",
      "Train size( Potato___Early_blight ):  600\n",
      "Validation size( Potato___Early_blight ):  200\n",
      "Test size( Potato___Early_blight ):  200\n",
      "Train size( Potato___healthy ):  91\n",
      "Validation size( Potato___healthy ):  30\n",
      "Test size( Potato___healthy ):  30\n",
      "Train size( Potato___Late_blight ):  600\n",
      "Validation size( Potato___Late_blight ):  200\n",
      "Test size( Potato___Late_blight ):  200\n",
      "Train size( Strawberry___healthy ):  273\n",
      "Validation size( Strawberry___healthy ):  91\n",
      "Test size( Strawberry___healthy ):  91\n",
      "Train size( Strawberry___Leaf_scorch ):  665\n",
      "Validation size( Strawberry___Leaf_scorch ):  221\n",
      "Test size( Strawberry___Leaf_scorch ):  221\n",
      "Train size( Tomato___Bacterial_spot ):  1276\n",
      "Validation size( Tomato___Bacterial_spot ):  425\n",
      "Test size( Tomato___Bacterial_spot ):  425\n",
      "Train size( Tomato___Early_blight ):  600\n",
      "Validation size( Tomato___Early_blight ):  200\n",
      "Test size( Tomato___Early_blight ):  200\n",
      "Train size( Tomato___healthy ):  954\n",
      "Validation size( Tomato___healthy ):  318\n",
      "Test size( Tomato___healthy ):  318\n",
      "Train size( Tomato___Late_blight ):  1145\n",
      "Validation size( Tomato___Late_blight ):  381\n",
      "Test size( Tomato___Late_blight ):  381\n",
      "Train size( Tomato___Leaf_Mold ):  571\n",
      "Validation size( Tomato___Leaf_Mold ):  190\n",
      "Test size( Tomato___Leaf_Mold ):  190\n",
      "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
      "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
      "Test size( Tomato___Septoria_leaf_spot ):  354\n",
      "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
      "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Train size( Tomato___Target_Spot ):  842\n",
      "Validation size( Tomato___Target_Spot ):  280\n",
      "Test size( Tomato___Target_Spot ):  280\n",
      "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
      "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
      "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
      "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할과 클래스별 데이터 수 확인\n",
    "import math\n",
    "\n",
    "for clss in classes_list:\n",
    "    path = os.path.join(original_dataset_dir, clss)\n",
    "    fnames = os.listdir(path)\n",
    "\n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    validation_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)\n",
    "\n",
    "    train_fnames = fnames[:train_size]\n",
    "    print('Train size(',clss,'): ', len(train_fnames))\n",
    "    for fname in train_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(train_dir, clss), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
    "    print('Validation size(',clss,'): ', len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(validation_dir, clss), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    test_fnames = fnames[(train_size + validation_size): (validation_size + train_size + test_size)]\n",
    "    print('Test size(',clss,'): ', len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(test_dir, clss), fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef43bd0-8e21-409f-84dc-371585ecddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인 모델 학습을 위한 준비\n",
    "import torch\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 30\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base)\n",
    "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb80c45-79a0-472b-9f83-0807a5a9b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인 모델 설계\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 33)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model_base = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1723b039-158a-4ff1-a861-d5796487fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습을 위한 함수\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba81cbaa-3be0-460b-9ca8-00577b0fb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가를 위한 함수\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da960b5-2612-4c1d-b678-7d45f0779d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 --------------\n",
      "train Loss: 0.9389, Accuracy: 70.77%\n",
      "val Loss: 0.9945, Accuracy: 68.91%\n",
      "Completed in 1m 4s\n",
      "-------------- epoch 2 --------------\n",
      "train Loss: 0.6519, Accuracy: 79.94%\n",
      "val Loss: 0.7087, Accuracy: 78.13%\n",
      "Completed in 0m 56s\n",
      "-------------- epoch 3 --------------\n",
      "train Loss: 0.5692, Accuracy: 81.89%\n",
      "val Loss: 0.6303, Accuracy: 79.67%\n",
      "Completed in 0m 56s\n",
      "-------------- epoch 4 --------------\n",
      "train Loss: 0.4716, Accuracy: 85.06%\n",
      "val Loss: 0.5405, Accuracy: 82.26%\n",
      "Completed in 0m 56s\n",
      "-------------- epoch 5 --------------\n",
      "train Loss: 0.4170, Accuracy: 86.86%\n",
      "val Loss: 0.4906, Accuracy: 84.40%\n",
      "Completed in 0m 58s\n",
      "-------------- epoch 6 --------------\n",
      "train Loss: 0.3680, Accuracy: 88.17%\n",
      "val Loss: 0.4523, Accuracy: 85.39%\n",
      "Completed in 1m 3s\n",
      "-------------- epoch 7 --------------\n",
      "train Loss: 0.3447, Accuracy: 89.15%\n",
      "val Loss: 0.4284, Accuracy: 86.04%\n",
      "Completed in 0m 58s\n",
      "-------------- epoch 8 --------------\n",
      "train Loss: 0.3023, Accuracy: 90.99%\n",
      "val Loss: 0.3968, Accuracy: 87.31%\n",
      "Completed in 0m 55s\n",
      "-------------- epoch 9 --------------\n",
      "train Loss: 0.2683, Accuracy: 91.84%\n",
      "val Loss: 0.3580, Accuracy: 88.33%\n",
      "Completed in 0m 55s\n",
      "-------------- epoch 10 --------------\n",
      "train Loss: 0.2443, Accuracy: 92.29%\n",
      "val Loss: 0.3410, Accuracy: 88.46%\n",
      "Completed in 0m 57s\n",
      "-------------- epoch 11 --------------\n",
      "train Loss: 0.2366, Accuracy: 92.72%\n",
      "val Loss: 0.3389, Accuracy: 89.02%\n",
      "Completed in 0m 55s\n",
      "-------------- epoch 12 --------------\n",
      "train Loss: 0.2056, Accuracy: 94.19%\n",
      "val Loss: 0.3058, Accuracy: 90.21%\n",
      "Completed in 0m 55s\n",
      "-------------- epoch 13 --------------\n",
      "train Loss: 0.1788, Accuracy: 95.03%\n",
      "val Loss: 0.2826, Accuracy: 91.28%\n",
      "Completed in 0m 55s\n",
      "-------------- epoch 14 --------------\n",
      "train Loss: 0.2064, Accuracy: 93.52%\n",
      "val Loss: 0.3176, Accuracy: 89.39%\n",
      "Completed in 0m 55s\n",
      "-------------- epoch 15 --------------\n",
      "train Loss: 0.1546, Accuracy: 95.43%\n",
      "val Loss: 0.2658, Accuracy: 91.66%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 16 --------------\n",
      "train Loss: 0.1620, Accuracy: 95.41%\n",
      "val Loss: 0.2793, Accuracy: 91.13%\n",
      "Completed in 0m 55s\n",
      "-------------- epoch 17 --------------\n",
      "train Loss: 0.1305, Accuracy: 96.37%\n",
      "val Loss: 0.2522, Accuracy: 92.01%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 18 --------------\n",
      "train Loss: 0.1247, Accuracy: 96.75%\n",
      "val Loss: 0.2435, Accuracy: 92.23%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 19 --------------\n",
      "train Loss: 0.1246, Accuracy: 96.44%\n",
      "val Loss: 0.2504, Accuracy: 92.13%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 20 --------------\n",
      "train Loss: 0.1146, Accuracy: 96.93%\n",
      "val Loss: 0.2421, Accuracy: 92.10%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 21 --------------\n",
      "train Loss: 0.1537, Accuracy: 95.40%\n",
      "val Loss: 0.2989, Accuracy: 90.46%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 22 --------------\n",
      "train Loss: 0.0973, Accuracy: 97.17%\n",
      "val Loss: 0.2335, Accuracy: 92.54%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 23 --------------\n",
      "train Loss: 0.0887, Accuracy: 97.60%\n",
      "val Loss: 0.2241, Accuracy: 92.60%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 24 --------------\n",
      "train Loss: 0.0794, Accuracy: 97.83%\n",
      "val Loss: 0.2197, Accuracy: 92.90%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 25 --------------\n",
      "train Loss: 0.0974, Accuracy: 97.17%\n",
      "val Loss: 0.2416, Accuracy: 92.24%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 26 --------------\n",
      "train Loss: 0.1225, Accuracy: 96.37%\n",
      "val Loss: 0.2697, Accuracy: 91.01%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 27 --------------\n",
      "train Loss: 0.0698, Accuracy: 98.08%\n",
      "val Loss: 0.2153, Accuracy: 93.14%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 28 --------------\n",
      "train Loss: 0.0657, Accuracy: 98.34%\n",
      "val Loss: 0.2037, Accuracy: 93.50%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 29 --------------\n",
      "train Loss: 0.0654, Accuracy: 98.32%\n",
      "val Loss: 0.2132, Accuracy: 93.10%\n",
      "Completed in 0m 54s\n",
      "-------------- epoch 30 --------------\n",
      "train Loss: 0.1189, Accuracy: 96.14%\n",
      "val Loss: 0.2756, Accuracy: 90.94%\n",
      "Completed in 0m 54s\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 실행하기\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs = 30):\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        since = time.time()\n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('-------------- epoch {} --------------'.format(epoch))\n",
    "\n",
    "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
    "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
    "\n",
    "torch.save(base, 'baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6f35ee4-bac9-47d0-a766-8b38589e8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning을 위한 준비\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = './splitted'\n",
    "image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b514d3f-59df-45a3-afc2-a4d6b4ddf38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIWON\\anaconda3\\envs\\speech\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\JIWON\\anaconda3\\envs\\speech\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\JIWON/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:01<00:00, 58.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Pre-Trained Model 불러오기\n",
    "from torchvision import models\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 33)\n",
    "resnet = resnet.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd87e75-f87c-4dc1-a25e-30d8fd3c4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Trained Model의 일부 Layer Freeze하기\n",
    "ct = 0\n",
    "for child in resnet.children():\n",
    "    ct += 1\n",
    "    if ct < 6:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4540afe8-cd17-4543-a7de-f569a7f54d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning 모델 학습과 검증을 위한 함수\n",
    "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------- epoch {} --------------'.format(epoch+1))\n",
    "        since = time.time()\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                l_r = [x['lr'] for x in optimizer_ft.param_groups]\n",
    "                print('learning rate: ',l_r)\n",
    "\n",
    "            epoch_loss = running_loss/dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46921531-c765-4f41-827b-afc5ab6d31d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 --------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.6144 Acc: 0.8171\n",
      "val Loss: 0.3774 Acc: 0.8777\n",
      "Completed in 0m 41s\n",
      "-------------- epoch 2 --------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.2240 Acc: 0.9281\n",
      "val Loss: 0.2370 Acc: 0.9240\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 3 --------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.1667 Acc: 0.9466\n",
      "val Loss: 0.1632 Acc: 0.9450\n",
      "Completed in 0m 41s\n",
      "-------------- epoch 4 --------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.1365 Acc: 0.9563\n",
      "val Loss: 0.1366 Acc: 0.9538\n",
      "Completed in 0m 41s\n",
      "-------------- epoch 5 --------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.1207 Acc: 0.9616\n",
      "val Loss: 0.1831 Acc: 0.9427\n",
      "Completed in 0m 41s\n",
      "-------------- epoch 6 --------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.1052 Acc: 0.9657\n",
      "val Loss: 0.1682 Acc: 0.9454\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 7 --------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0920 Acc: 0.9697\n",
      "val Loss: 0.1346 Acc: 0.9574\n",
      "Completed in 0m 45s\n",
      "-------------- epoch 8 --------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0470 Acc: 0.9845\n",
      "val Loss: 0.0423 Acc: 0.9867\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 9 --------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0288 Acc: 0.9912\n",
      "val Loss: 0.0380 Acc: 0.9880\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 10 --------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0259 Acc: 0.9918\n",
      "val Loss: 0.0381 Acc: 0.9875\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 11 --------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0222 Acc: 0.9925\n",
      "val Loss: 0.0367 Acc: 0.9874\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 12 --------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0215 Acc: 0.9927\n",
      "val Loss: 0.0345 Acc: 0.9886\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 13 --------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.0185 Acc: 0.9942\n",
      "val Loss: 0.0351 Acc: 0.9890\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 14 --------------\n",
      "learning rate:  [1e-05]\n",
      "train Loss: 0.0170 Acc: 0.9947\n",
      "val Loss: 0.0328 Acc: 0.9895\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 15 --------------\n",
      "learning rate:  [1e-05]\n",
      "train Loss: 0.0154 Acc: 0.9950\n",
      "val Loss: 0.0310 Acc: 0.9904\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 16 --------------\n",
      "learning rate:  [1e-05]\n",
      "train Loss: 0.0150 Acc: 0.9949\n",
      "val Loss: 0.0278 Acc: 0.9915\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 17 --------------\n",
      "learning rate:  [1e-05]\n",
      "train Loss: 0.0147 Acc: 0.9950\n",
      "val Loss: 0.0272 Acc: 0.9900\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 18 --------------\n",
      "learning rate:  [1e-05]\n",
      "train Loss: 0.0139 Acc: 0.9951\n",
      "val Loss: 0.0310 Acc: 0.9895\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 19 --------------\n",
      "learning rate:  [1e-05]\n",
      "train Loss: 0.0126 Acc: 0.9959\n",
      "val Loss: 0.0274 Acc: 0.9906\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 20 --------------\n",
      "learning rate:  [1e-05]\n",
      "train Loss: 0.0150 Acc: 0.9949\n",
      "val Loss: 0.0286 Acc: 0.9912\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 21 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train Loss: 0.0133 Acc: 0.9960\n",
      "val Loss: 0.0304 Acc: 0.9905\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 22 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train Loss: 0.0121 Acc: 0.9962\n",
      "val Loss: 0.0259 Acc: 0.9914\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 23 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train Loss: 0.0125 Acc: 0.9960\n",
      "val Loss: 0.0307 Acc: 0.9905\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 24 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train Loss: 0.0134 Acc: 0.9960\n",
      "val Loss: 0.0292 Acc: 0.9907\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 25 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train Loss: 0.0127 Acc: 0.9956\n",
      "val Loss: 0.0276 Acc: 0.9920\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 26 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train Loss: 0.0133 Acc: 0.9958\n",
      "val Loss: 0.0276 Acc: 0.9900\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 27 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train Loss: 0.0120 Acc: 0.9964\n",
      "val Loss: 0.0281 Acc: 0.9911\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 28 --------------\n",
      "learning rate:  [1.0000000000000002e-07]\n",
      "train Loss: 0.0143 Acc: 0.9953\n",
      "val Loss: 0.0274 Acc: 0.9910\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 29 --------------\n",
      "learning rate:  [1.0000000000000002e-07]\n",
      "train Loss: 0.0117 Acc: 0.9962\n",
      "val Loss: 0.0305 Acc: 0.9905\n",
      "Completed in 0m 42s\n",
      "-------------- epoch 30 --------------\n",
      "learning rate:  [1.0000000000000002e-07]\n",
      "train Loss: 0.0128 Acc: 0.9957\n",
      "val Loss: 0.0285 Acc: 0.9905\n",
      "Completed in 0m 42s\n",
      "Best val Acc: 0.9920\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 실행하기\n",
    "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)\n",
    "\n",
    "torch.save(model_resnet50, 'resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b46f8396-8372-428b-96fb-62a158119eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인 모델 평가를 위한 전처리\n",
    "transform_base = transforms.Compose([transforms.Resize([64,64]), transforms.ToTensor()])\n",
    "test_base = ImageFolder(root='./splitted/test', transform=transform_base)\n",
    "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48a94688-a92c-4433-88d7-6d55f9915db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning모델 평가를 위한 전처리\n",
    "transform_resNet = transforms.Compose([\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet)\n",
    "\n",
    "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afe10cf8-7ba1-43b0-ac54-34d9e6db6527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIWON\\AppData\\Local\\Temp\\ipykernel_18616\\2648250400.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  baseline=torch.load('baseline.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test acc:  93.42846413819001\n"
     ]
    }
   ],
   "source": [
    "# 베이스라인 모델 평가하기\n",
    "baseline=torch.load('baseline.pt')\n",
    "baseline.eval()\n",
    "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
    "\n",
    "print('baseline test acc: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f62efb-e295-4547-9a6a-9f8edf6893cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JIWON\\AppData\\Local\\Temp\\ipykernel_18616\\1858529960.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet50=torch.load('resnet50.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet test acc:  99.04869195143323\n"
     ]
    }
   ],
   "source": [
    "# Transfer Learning 모델 성능 평가하기\n",
    "resnet50=torch.load('resnet50.pt')\n",
    "resnet50.eval()\n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
    "\n",
    "print('ResNet test acc: ', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
